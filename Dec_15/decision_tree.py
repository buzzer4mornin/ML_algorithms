#!/usr/bin/env python3
import argparse

import numpy as np
import sklearn.datasets
import sklearn.metrics
import sklearn.model_selection

parser = argparse.ArgumentParser()
# These arguments will be set appropriately by ReCodEx, even if you change them.
parser.add_argument("--criterion", default="gini", type=str, help="Criterion to use; either `gini` or `entropy`")
parser.add_argument("--max_depth", default=None, type=int, help="Maximum decision tree depth")
parser.add_argument("--max_leaves", default=None, type=int, help="Maximum number of leaf nodes")
parser.add_argument("--min_to_split", default=2, type=int, help="Minimum examples required to split")
parser.add_argument("--recodex", default=False, action="store_true", help="Running in ReCodEx")
parser.add_argument("--seed", default=42, type=int, help="Random seed")
parser.add_argument("--test_size", default=42, type=lambda x:int(x) if x.isdigit() else float(x), help="Test set size")
# If you add more arguments, ReCodEx will keep them with your default values.

def main(args):
    # Use the wine dataset
    data, target = sklearn.datasets.load_wine(return_X_y=True)

    # Split the data randomly to train and test using `sklearn.model_selection.train_test_split`,
    # with `test_size=args.test_size` and `random_state=args.seed`.
    train_data, test_data, train_target, test_target = sklearn.model_selection.train_test_split(
        data, target, test_size=args.test_size, random_state=args.seed)

    class DecisionTreeClassifier:
        def __init__(self, max_depth=None):
            self.max_depth = max_depth

        def _best_split(self, X, y):
            """Find the best split for a node.
            "Best" means that the average impurity of the two children, weighted by their
            population, is the smallest possible. Additionally it must be less than the
            impurity of the current node.
            To find the best split, we loop through all the features, and consider all the
            midpoints between adjacent training samples as possible thresholds. We compute
            the Gini impurity of the split generated by that particular feature/threshold
            pair, and return the pair with smallest impurity.
            Returns:
                best_idx: Index of the feature for best split, or None if no split is found.
                best_thr: Threshold to use for the split, or None if no split is found.
            """
            # Need at least two elements to split a node.
            m = y.size
            if m <= 1:
                return None, None

            # Count of each class in the current node.
            num_parent = [np.sum(y == c) for c in range(self.n_classes_)]

            # Gini of current node.
            best_gini = 1.0 - sum((n / m) ** 2 for n in num_parent)
            best_idx, best_thr = None, None

            # Loop through all features.
            for idx in range(self.n_features_):
                # Sort data along selected feature.
                thresholds, classes = zip(*sorted(zip(X[:, idx], y)))

                # We could actually split the node according to each feature/threshold pair
                # and count the resulting population for each class in the children, but
                # instead we compute them in an iterative fashion, making this for loop
                # linear rather than quadratic.
                num_left = [0] * self.n_classes_
                num_right = num_parent.copy()
                for i in range(1, m):  # possible split positions
                    c = classes[i - 1]
                    num_left[c] += 1
                    num_right[c] -= 1
                    gini_left = 1.0 - sum(
                        (num_left[x] / i) ** 2 for x in range(self.n_classes_)
                    )
                    gini_right = 1.0 - sum(
                        (num_right[x] / (m - i)) ** 2 for x in range(self.n_classes_)
                    )

                    # The Gini impurity of a split is the weighted average of the Gini
                    # impurity of the children.
                    gini = (i * gini_left + (m - i) * gini_right) / m

                    # The following condition is to make sure we don't try to split two
                    # points with identical values for that feature, as it is impossible
                    # (both have to end up on the same side of a split).
                    if thresholds[i] == thresholds[i - 1]:
                        continue

                    if gini < best_gini:
                        best_gini = gini
                        best_idx = idx
                        best_thr = (thresholds[i] + thresholds[i - 1]) / 2  # midpoint

            return best_idx, best_thr






    # TODO: Create a decision tree on the trainining data.
    #
    # - For each node, predict the most frequent class (and the one with
    #   smallest index if there are several such classes).
    #
    # - When splitting a node, consider the features in sequential order, then
    #   for each feature consider all possible split points ordered in ascending
    #   value, and perform the first encountered split descreasing the criterion
    #   the most. Each split point is an average of two nearest unique feature values
    #   of the instances corresponding to the given node (i.e., for four instances
    #   with values 1, 7, 3, 3 the split points are 2 and 5).
    #
    # - Allow splitting a node only if:
    #   - when `args.max_depth` is not None, its depth must be less than `args.max_depth`;
    #     depth of the root node is zero;
    #   - there are at least `args.min_to_split` corresponding instances;
    #   - the criterion value is not zero.
    #
    # - When `args.max_leaves` is None, use recursive (left descendants first, then
    #   right descendants) approach, splitting every node if the constraints are valid.
    #   Otherwise (when `args.max_leaves` is not None), always split a node where the
    #   constraints are valid and the overall criterion value (c_left + c_right - c_node)
    #   decreases the most. If there are several such nodes, choose the one
    #   which was created sooner (a left child is considered to be created
    #   before a right child).

    # TODO: Finally, measure the training and testing accuracy.
    train_accuracy = None
    test_accuracy = None

    return train_accuracy, test_accuracy

if __name__ == "__main__":
    args = parser.parse_args([] if "__file__" not in globals() else None)
    train_accuracy, test_accuracy = main(args)

    #print("Train accuracy: {:.1f}%".format(100 * train_accuracy))
    #print("Test accuracy: {:.1f}%".format(100 * test_accuracy))